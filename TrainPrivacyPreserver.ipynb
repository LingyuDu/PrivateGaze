{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f14dc9-0a3f-48d4-99da-89031dccf00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.io import read_image\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torch.nn as nn\n",
    "import load_dataset\n",
    "import common_functions\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import unet_model\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_msssim import MS_SSIM, ms_ssim, SSIM, ssim\n",
    "import unet_model_v2\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e7320-d985-4e0c-b978-c188bbc2158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MS_SSIM_Loss(MS_SSIM):\n",
    "    def forward(self, img1, img2):\n",
    "        return 75*( 1 - super(MS_SSIM_Loss, self).forward(img1, img2) )\n",
    "\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.pooling = torch.nn.AvgPool2d(28)\n",
    "        self.layer1 = torch.nn.Linear(256, 128)\n",
    "        self.layer2 = torch.nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.Flatten()(self.pooling(x))\n",
    "        output = self.layer2(F.relu(self.layer1(x)))\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = 4\n",
    "BATCHSIZE = 32\n",
    "\n",
    "GazeCapture = load_dataset.GazeCapture()\n",
    "size = len(GazeCapture)\n",
    "train_size = int(size * 0.8)\n",
    "test_size = size - train_size\n",
    "mpii_data = load_dataset.MPIIDataset()\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(GazeCapture, [train_size, test_size],\n",
    "                                                            generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCHSIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_dataloader = DataLoader(mpii_data, batch_size=BATCHSIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "Encoder_raw = unet_model_v2.UNetEncoder(n_channels=3, num_classes=3, base_filter_num=32, num_blocks=4).to(device)\n",
    "Encoder_anchor = unet_model_v2.UNetEncoder(n_channels=3, num_classes=3, base_filter_num=32, num_blocks=4).to(device)\n",
    "Decoder = unet_model_v2.UNetDecoder(n_channels=3, num_classes=3, base_filter_num=32, num_blocks=4).to(device)\n",
    "gaze_mlp = MLP().to(device)\n",
    "\n",
    "gaze_estimator = resnet18().to(device)\n",
    "gaze_estimator.fc = nn.Linear(512, 2).to(device)\n",
    "gaze_estimator.load_state_dict(torch.load(\"/data/volume_2/GazePrivacyModelsV2/PretrainRes18GazeCapture/model.pt\"))\n",
    "\n",
    "\n",
    "identity_net = resnet18().to(device)\n",
    "identity_net.fc = nn.Linear(512, 15).to(device)\n",
    "\n",
    "loss_reconstruction = MS_SSIM_Loss(data_range=1.0, size_average=True, channel=3)\n",
    "loss_gaze = nn.L1Loss()\n",
    "loss_idenetity = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ae = torch.optim.Adam(list(Encoder_raw.parameters())+list(Encoder_anchor.parameters())\n",
    "                                + list(Decoder.parameters()) + list(gaze_mlp.parameters()), lr=1e-3)\n",
    "optimizer_id = torch.optim.Adam(identity_net.parameters(), lr=5e-3)\n",
    "optimizer_de = torch.optim.Adam(Decoder.parameters(), lr=1e-3)\n",
    "\n",
    "anchor_image = torch.unsqueeze(read_image(\"/data/volume_2/Gaze_privacy_v2/Gaze_privacy/gazecapture_average_face_efficientnet.png\")/255., 0).to(device)\n",
    "anchor_image = F.interpolate(anchor_image, size=(224, 224))\n",
    "\n",
    "gaze_model_vgg = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=False).to(device)\n",
    "gaze_model_vgg.classifier._modules['6'] = nn.Linear(4096, 2).to(device)\n",
    "gaze_model_vgg.load_state_dict(torch.load(\"/data/volume_2/GazePrivacyModels/PretrainVGG11XGaze/model.pt\"))\n",
    "\n",
    "gaze_model_res18 = resnet18().to(device)\n",
    "gaze_model_res18.fc = nn.Linear(512, 2).to(device)\n",
    "gaze_model_res18.load_state_dict(torch.load(\"/data/volume_2/GazePrivacyModels/PretrainRes18XGaze/model.pt\"))\n",
    "\n",
    "gaze_model_mobilenet = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True).to(device)\n",
    "gaze_model_mobilenet.classifier[1] = nn.Linear(1280, 2).to(device)\n",
    "gaze_model_mobilenet.load_state_dict(torch.load(\"/data/volume_2/GazePrivacyModels/PretrainMobileNetV2XGaze/model.pt\"))\n",
    "\n",
    "gaze_model_efficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True).to(device)\n",
    "gaze_model_efficientnet.classifier.fc = nn.Linear(1280, 2).to(device)\n",
    "gaze_model_efficientnet.load_state_dict(torch.load(\"/data/volume_2/GazePrivacyModels/PretrainEffiNetXGaze/model.pt\"))\n",
    "\n",
    "gaze_model_shufflenet = torch.hub.load('pytorch/vision:v0.10.0', 'shufflenet_v2_x1_0', pretrained=True).to(device)\n",
    "gaze_model_shufflenet.fc = nn.Linear(1024, 2).to(device)\n",
    "gaze_model_shufflenet.load_state_dict(torch.load(\"/data/volume_2/GazePrivacyModels/PretrainShuffleNetXGaze/model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6244b271-6dbc-4cd1-939d-7bbddce8e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID_train(dataloader, ID):\n",
    "    size = len(dataloader.dataset)\n",
    "    Encoder_raw.eval()\n",
    "    Encoder_anchor.eval()\n",
    "    Decoder.eval()\n",
    "    ID.train()\n",
    "\n",
    "    for batch, (X, y) in tqdm(enumerate(dataloader)):\n",
    "        X, y, y_id = X.to(device), y[:, 0:2].to(device), y[:, 2]\n",
    "        y_id = y_id.type(torch.LongTensor).to(device)\n",
    "\n",
    "        representation, _ = Encoder_raw(X)\n",
    "        _, anchor_mid = Encoder_anchor(anchor_image)\n",
    "        anchor_mid = [torch.tile(anchor_mid[0], (y_id.shape[0], 1, 1, 1)), torch.tile(anchor_mid[1], (y_id.shape[0], 1, 1, 1)),\n",
    "                      torch.tile(anchor_mid[2], (y_id.shape[0], 1, 1, 1)), torch.tile(anchor_mid[3], (y_id.shape[0], 1, 1, 1))]\n",
    "        reconstructed_img = Decoder(representation, anchor_mid)\n",
    "\n",
    "        id_pre = ID(reconstructed_img)\n",
    "        loss = loss_idenetity(id_pre, y_id)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_id.step()\n",
    "        optimizer_id.zero_grad()\n",
    "\n",
    "        if batch % 300 == 0:\n",
    "            current = (batch + 1) * len(X)\n",
    "            print(f\"[{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "            id_pre = ID(reconstructed_img)\n",
    "            _, predictions = torch.max(id_pre, 1)\n",
    "            correct = (predictions == y_id).sum().item()\n",
    "            print(\"id acc\", correct / BATCHSIZE)\n",
    "            img = reconstructed_img.cpu().data.numpy()[0, :, :, :]\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "            plt.imshow(img)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c7bd5-05de-41c4-8ada-858bc847fe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, Gaze, ID):\n",
    "    size = len(dataloader.dataset)\n",
    "    Gaze.eval()\n",
    "    Encoder_raw.eval()\n",
    "    Encoder_anchor.eval()\n",
    "    Decoder.eval()\n",
    "    ID.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y, y_id = X.to(device), y[:, 0:2].to(device), y[:, 2]\n",
    "            y_id = y_id.type(torch.LongTensor).to(device)\n",
    "\n",
    "            representation, _ = Encoder_raw(X)\n",
    "            _, anchor_mid = Encoder_anchor(anchor_image)\n",
    "            anchor_mid = [torch.tile(anchor_mid[0], (y_id.shape[0], 1, 1, 1)),\n",
    "                          torch.tile(anchor_mid[1], (y_id.shape[0], 1, 1, 1)),\n",
    "                          torch.tile(anchor_mid[2], (y_id.shape[0], 1, 1, 1)),\n",
    "                          torch.tile(anchor_mid[3], (y_id.shape[0], 1, 1, 1))]\n",
    "            reconstructed_img = Decoder(representation, anchor_mid)\n",
    "\n",
    "            gaze_pre = Gaze(reconstructed_img)\n",
    "            test_loss += common_functions.avg_angle_error(gaze_pre, y).item() * y.shape[0]\n",
    "\n",
    "            id_pre = ID(reconstructed_img)\n",
    "            _, predictions = torch.max(id_pre, 1)\n",
    "            correct += (predictions == y_id).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    print(\"angular error on testing set:\", test_loss, \"id acc\", correct / size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e88418-9aa9-44ec-b785-434dedf95416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train_ae(dataloader, Gaze, ID):\n",
    "    size = len(dataloader.dataset)\n",
    "    Encoder_raw.train()\n",
    "    Encoder_anchor.train()\n",
    "    Decoder.train()\n",
    "    ID.eval()\n",
    "    Gaze.eval()\n",
    "\n",
    "    for batch, (X, y) in tqdm(enumerate(dataloader)):\n",
    "        X, y, y_id = X.to(device), y[:, 0:2].to(device), y[:, 2]\n",
    "        y_id = y_id.type(torch.LongTensor).to(device)\n",
    "        # y = y + bias_prediction\n",
    "        representation, _ = Encoder_raw(X)\n",
    "        _, anchor_mid = Encoder_anchor(anchor_image)\n",
    "        anchor_mid = [torch.tile(anchor_mid[0], (y_id.shape[0], 1, 1, 1)), torch.tile(anchor_mid[1], (y_id.shape[0], 1, 1, 1)),\n",
    "                      torch.tile(anchor_mid[2], (y_id.shape[0], 1, 1, 1)), torch.tile(anchor_mid[3], (y_id.shape[0], 1, 1, 1))]\n",
    "        reconstructed_img = Decoder(representation, anchor_mid)\n",
    "\n",
    "        gaze_pre = Gaze(reconstructed_img)\n",
    "        gaze_pre_mlp = gaze_mlp(representation)\n",
    "        loss = loss_gaze(gaze_pre, y) + loss_gaze(gaze_pre_mlp, y) + \\\n",
    "               loss_reconstruction(reconstructed_img, torch.tile(anchor_image, (y_id.shape[0], 1, 1, 1)))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_ae.step()\n",
    "        optimizer_ae.zero_grad()\n",
    "        \n",
    "        \n",
    "        if batch % 300 == 0:\n",
    "            current = (batch + 1) * len(X)\n",
    "            print(f\"[{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "            loss_recon = loss_reconstruction(reconstructed_img, X)\n",
    "            gaze_pre = Gaze(reconstructed_img)\n",
    "            gaze_pre_mlp = gaze_mlp(representation)\n",
    "            #converted_gaze_pre_mlp = gaze_mlp(converted_representation)\n",
    "            angle_error = common_functions.avg_angle_error(gaze_pre, y)\n",
    "            angle_error_mlp = common_functions.avg_angle_error(gaze_pre_mlp, y)\n",
    "            #converted_angle_error_mlp = common_functions.avg_angle_error(converted_gaze_pre_mlp, y)\n",
    "            id_pre = ID(reconstructed_img)\n",
    "            _, predictions = torch.max(id_pre, 1)\n",
    "            correct = (predictions == y_id).sum().item()\n",
    "            print(\"reconstruction loss\", loss_recon.item(), \"gaze loss\", angle_error.item(),\n",
    "                  \"gaze loss mlp\", angle_error_mlp.item(), #\"converted gaze loss mlp\", converted_angle_error_mlp.item(), \n",
    "                  \"id acc\", correct / BATCHSIZE)\n",
    "            img = reconstructed_img.cpu().data.numpy()[0, :, :, :]\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "        if (batch % 3000 == 0 and batch!=0) or batch == 300:\n",
    "            print(\"testing performance on whitebox gaze model\")\n",
    "            test(test_dataloader, gaze_estimator, identity_net)\n",
    "\n",
    "            print(\"testing performance on efficientnet blackbox gaze model (XGaze)\")\n",
    "            test(test_dataloader, gaze_model_efficientnet, identity_net)\n",
    "        if batch == 12000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f873c213-1fec-4b1b-b737-79a08545f8c1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    pre_train_ae(train_dataloader, gaze_estimator, identity_net)\n",
    "    \n",
    "    print(\"testing performance on whitebox gaze model\")\n",
    "    test(test_dataloader, gaze_estimator, identity_net)\n",
    "    \n",
    "    print(\"testing performance on vgg11 blackbox gaze model (XGaze)\")\n",
    "    test(test_dataloader, gaze_model_vgg, identity_net)\n",
    "    \n",
    "    print(\"testing performance on resnet18 blackbox gaze model (XGaze)\")\n",
    "    test(test_dataloader, gaze_model_res18, identity_net)\n",
    "    \n",
    "    print(\"testing performance on mobilenetv2 blackbox gaze model (XGaze)\")\n",
    "    test(test_dataloader, gaze_model_mobilenet, identity_net)\n",
    "    \n",
    "    print(\"testing performance on efficientnet blackbox gaze model (XGaze)\")\n",
    "    test(test_dataloader, gaze_model_efficientnet, identity_net)\n",
    "    \n",
    "    print(\"testing performance on shufflenet blackbox gaze model (XGaze)\")\n",
    "    test(test_dataloader, gaze_model_shufflenet, identity_net)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc5d1d1-4a34-4550-b021-14fe6784bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Encoder_raw.state_dict(), \"/data/volume_2/GazePrivacyModelsV2/GazeCaptureRes18EncRawMeanFace/efficientnet_avg_face.pt\")\n",
    "torch.save(Encoder_anchor.state_dict(), \"/data/volume_2/GazePrivacyModelsV2/GazeCaptureRes18EncAnchorMeanFace/efficientnet_avg_face.pt\")\n",
    "torch.save(Decoder.state_dict(), \"/data/volume_2/GazePrivacyModelsV2/GazeCaptureRes18DecTwoMeanFace/efficientnet_avg_face.pt\")\n",
    "torch.save(gaze_mlp.state_dict(), \"/data/volume_2/GazePrivacyModelsV2/GazeCaptureMLP/efficientnet_avg_face.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
